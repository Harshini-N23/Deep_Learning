{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Implement a Multilayer Perceptron for Data Classification using Tensorflow\n",
        "\n",
        "## Aim: To build a multilayer perceptron for the  classification of  Iris Species-Setosa, Versicolor and Virginica\n",
        "## Dataset : Iris\n"
      ],
      "metadata": {
        "id": "jj9xFu6GiaAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAaMH2vnGQt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2722d4-0d4e-4739-8f26-9265a3b56e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.compat.v1.global_variables_initializer()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(label):\n",
        "  value =[]\n",
        "  if label ==\"Iris-setosa\":\n",
        "    value = [1,0,0]\n",
        "  elif label ==\"Iris-versicolor\":\n",
        "    value = [0,1,0]\n",
        "  elif label ==\"Iris-virginica\":\n",
        "    value =[0,0,1]\n",
        "  return value\n",
        "\n",
        "\n",
        "def data_encode(file):\n",
        "  X = []\n",
        "  Y = []\n",
        "  train_file =open(\"/content2/drive/MyDrive/Colab Notebooks/DEEP LEARNING/iris.train\", \"r\")\n",
        "  for line in train_file.read().strip().split(\"\\n\"):\n",
        "    line = line.split(\",\")\n",
        "    X.append([line[0], line[1],line[2],line[3]])\n",
        "    Y.append(label_encoder(line[4]))\n",
        "  return X, Y\n",
        "\n",
        "def model(input, weights, bias):\n",
        "  layer_1 = tf.add(tf.matmul(input, weights[\"hidden\"]), bias[\"hidden\"])\n",
        "  layer_1 =tf.nn.relu(layer_1)\n",
        "  output_layer =tf.matmul(layer_1, weights[\"output\"])+bias[\"output\"]\n",
        "  return output_layer\n",
        "\n",
        "X_train, Y_train = data_encode(\"iris.train\")\n",
        "X_test, Y_test = data_encode(\"iris.test\")\n",
        "learning_rate =0.01\n",
        "training_epochs = 2000\n",
        "display_steps = 200\n",
        "n_input= 4\n",
        "n_hidden = 10\n",
        "n_output = 3\n",
        "\n",
        "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.compat.v1.placeholder(\"float\", [None, n_output])\n",
        "\n",
        "weights = {\n",
        "    \"hidden\": tf.Variable(tf.random.normal([n_input, n_hidden]), name =\"Weights_hidden\"),\n",
        "    \"output\": tf.Variable(tf.random.normal([n_hidden,n_output]), name = \"wieghts_output\")\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"hidden\":tf.Variable(tf.random.normal([n_hidden]), name=\"bias_hidden\"),\n",
        "    \"output\": tf.Variable(tf.random.normal([n_output]), name=\"bias_output\")\n",
        "}\n",
        "\n",
        "pred = model(X, weights, bias)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =pred, labels =Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(training_epochs):\n",
        "    i,c = sess.run([optimizer, cost], feed_dict={X:X_train, Y:Y_train})\n",
        "    if(epoch+1)% display_steps ==0:\n",
        "      print(\"Epoch\", epoch+1, \"Cost: \", c)\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "\n",
        "  test_result =sess.run(pred, feed_dict={X:X_test})\n",
        "  correct_pred =tf.equal(tf.argmax(test_result,1), tf.argmax(Y_train, 1))\n",
        "  accuracy =tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
        "  ac= accuracy.eval({X:X_test, Y:Y_test})\n",
        "  print(\"Accuracy: \", round(ac*100,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9xzoww4HFAM",
        "outputId": "5f1c3fca-fa59-438d-a02e-9ec9f9f3db71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200 Cost:  0.20005403\n",
            "Epoch 400 Cost:  0.15498608\n",
            "Epoch 600 Cost:  0.1317955\n",
            "Epoch 800 Cost:  0.117578425\n",
            "Epoch 1000 Cost:  0.10798618\n",
            "Epoch 1200 Cost:  0.10108706\n",
            "Epoch 1400 Cost:  0.09588959\n",
            "Epoch 1600 Cost:  0.09183343\n",
            "Epoch 1800 Cost:  0.08857835\n",
            "Epoch 2000 Cost:  0.08590703\n",
            "Optimization Finished\n",
            "Accuracy:  98.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: We can observe that as the number of epochs increased, the cost/loss has decreased considerably and the model has achieved an overall accuracy of 98%"
      ],
      "metadata": {
        "id": "CGpGVsMaHvaH"
      }
    }
  ]
}